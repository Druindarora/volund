# üîÑ Module singleton pour g√©rer le mod√®le Whisper dans Parlia

# Ce fichier garantit qu‚Äôun seul mod√®le Whisper est charg√© √† la fois.
# Toutes les op√©rations de transcription passent par ici.

from pathlib import Path
from typing import Optional

import whisper  # Assure-toi d‚Äôavoir `openai-whisper` install√© via `pip install -U openai-whisper`

from modules.parlia.services.parlia_state_manager import parlia_state

_current_model: Optional[whisper.Whisper] = None  # type: Optional[whisper.Whisper]


def load_model(model_path: str):
    """
    Charge un mod√®le Whisper, soit depuis un nom int√©gr√©, soit depuis un fichier dans le dossier utilisateur.
    """
    global _current_model

    if _current_model is not None:
        print("[INFO] Un mod√®le est d√©j√† charg√©. Ignorer la demande.")
        return

    # Cas 1 : mod√®le int√©gr√© (fourni par Whisper directement)
    if model_path in ["tiny", "base", "small", "medium", "large"]:
        print(f"[INFO] Chargement du mod√®le Whisper int√©gr√© : {model_path}")
        _current_model = whisper.load_model(model_path)

    else:
        # Cas 2 : mod√®le custom => r√©cup√©rer le dossier s√©lectionn√© par l'utilisateur
        from modules.parlia.services.parlia_data import get_model_folder_path

        model_dir = get_model_folder_path()

        if not model_dir:
            print(
                "[ERREUR] Aucun dossier mod√®le d√©fini dans les pr√©f√©rences utilisateur."
            )
            return

        full_path = Path(model_dir) / model_path

        if full_path.exists():
            print(
                f"[INFO] Chargement du mod√®le Whisper depuis fichier : {full_path.resolve()}"
            )
            _current_model = whisper.load_model(str(full_path))
        else:
            print(f"[ERREUR] Le mod√®le sp√©cifi√© est introuvable : {full_path}")
            return

    print(f"[INFO] ‚úÖ Mod√®le charg√© avec succ√®s : {model_path}")
    parlia_state.set_whisper_ready(True)


def unload_model():
    """
    D√©charge le mod√®le actuellement charg√©.
    """
    global _current_model

    if _current_model is None:
        print("[INFO] Aucun mod√®le √† d√©charger.")
        return

    print(f"[INFO] D√©chargement du mod√®le.")
    _current_model = None
    parlia_state.set_whisper_ready(False)


def is_model_loaded() -> bool:
    """
    Retourne True si un mod√®le est actuellement charg√©.
    """
    return _current_model is not None


def get_model():
    """
    Retourne l‚Äôinstance du mod√®le actuel.
    """
    return _current_model


def transcribe(audio_path: str) -> str:
    """
    Transcrit un fichier audio en texte via le mod√®le Whisper charg√©.
    """
    if _current_model is None:
        raise RuntimeError("Aucun mod√®le Whisper n'est charg√©.")

    print(f"[INFO] Lancement de la transcription r√©elle via Whisper.")
    result = _current_model.transcribe(audio_path)

    # Ajout d‚Äôun log pour v√©rification
    print("[DEBUG] R√©sultat brut de Whisper :", result)

    text = result.get("text", "")
    if isinstance(text, str):
        return text.strip()
    else:
        return ""
